{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "Current benchmarks like SWE-bench and SWE-bench Verified evaluate agents on a diverse set of real-world engineering tasks. However, they lack any structured representation of the task space itself. As a result, these headline benchmarks function more as scoreboards than as frameworks for systematically guiding agent development. Without insight into how tasks are distributed or how they span the feature space, it becomes difficult to diagnose where and why agents succeed or fail — limiting the benchmark's utility for capability-driven progress.\n",
        "\n",
        "# Research Question\n",
        "\n",
        "Can we map the feature space of benchmark tasks using clustering techniques for mixed-type data, in order to better understand agent successes and failures? Can this structure then be used to both guide the development of specific agent capabilities and more accurately measure agent performance?\n",
        "\n",
        "# Supported Sections: Results Unified Spherical Clustering\n",
        "This notebook supports Unified Spherical Clustering results in the Results section."
      ],
      "metadata": {
        "id": "YY62aiIEzZcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import normalize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import normalized_mutual_info_score\n"
      ],
      "metadata": {
        "id": "_OSg3SOmIUF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysing the Annotation Dataset"
      ],
      "metadata": {
        "id": "O77a1eN58LDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotate = pd.read_csv(\"ensembled_annotations_public.csv\")\n"
      ],
      "metadata": {
        "id": "P5nZazXopyEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annotate.head()"
      ],
      "metadata": {
        "id": "SNKptee_snud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SWE-Bench Verified Dataset"
      ],
      "metadata": {
        "id": "4In5lZvh_5XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##dataset downloaded from huggingface\n",
        "\n",
        "df=pd.read_csv(\"swe_bench_verified_test.csv\")\n",
        "df_merged = df.merge(df_annotate, on='instance_id', how='left')"
      ],
      "metadata": {
        "id": "zTnjQTqcsa-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "id": "IFK5OC112DYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping duplicate columns and columns that provide the index number of people who made a given decision\n",
        "cols_to_drop = [\"difficulty_y\"] + [col for col in df_merged.columns if col.endswith('_decided_by')] + \\\n",
        "              [\"filter_out\"] + [\"difficulty_ensemble_decision_procedure\"] + \\\n",
        "               [\"other_notes\"] +[\"other_major_issues\"] + [\"instance_id\"] +[\"created_at\"]+[\"version\"] + \\\n",
        "               [\"environment_setup_commit\"] + [\"base_commit\"] +[\"test_patch\"]\n",
        "df_cleaned = df_merged.drop(columns = cols_to_drop)\n",
        "df_cleaned = df_cleaned.rename(columns = {\"difficulty_x\":\"difficulty\"})"
      ],
      "metadata": {
        "id": "RjgI35tj2JTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "Wzkf-J7gH2Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction From Patches\n",
        "\n",
        "To prepare the dataset for clustering, I will extract the structural features from each task's associated pull request using the PatchSet class from the unidiff Python library.\n",
        "\n",
        "A patch here refers to a code change and is also commonly referred to as a  diff — the code changes proposed to solve the benchmark task. PatchSet parses the unified diff format and allows for the extraction of a number of low-level structural signals from each patch.\n",
        "\n",
        "The features extracted were:\n",
        "\n",
        "- The number of files changed\n",
        "\n",
        "- The number of hunks in a given code change (i.e., blocks of contiguous edits)\n",
        "\n",
        "- The number of lines of code added\n",
        "\n",
        "- The number of lines of code removed\n",
        "\n",
        "These features serve as numerical proxies for task size and task complexity and provide a rough measure of how involved a code change is. From these derived features will be extracted also. This will be important later for mapping to agent capabilities like localisation (where in the code to make the edits) and planning (how many steps are needed).\n"
      ],
      "metadata": {
        "id": "Gb8N93NSA07P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidiff\n",
        "\n",
        "from unidiff import PatchSet\n"
      ],
      "metadata": {
        "id": "GSJBeyyo9DEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_patch_features(patch_text: str) -> pd.Series:\n",
        "    \"\"\"Function to extract basic numerical features from patch and compute derived features.\"\"\"\n",
        "    try:\n",
        "        patch = PatchSet(patch_text)\n",
        "\n",
        "        #Basic features\n",
        "        num_files = len(patch)\n",
        "        num_hunks = sum(len(file) for file in patch)\n",
        "        lines_added = sum(hunk.added for file in patch for hunk in file)\n",
        "        lines_removed = sum(hunk.removed for file in patch for hunk in file)\n",
        "\n",
        "        #Derived Features\n",
        "        total_lines_changed = lines_added + lines_removed\n",
        "        change_ratio = np.log1p(lines_added) / (np.log1p(lines_removed) + 1)\n",
        "\n",
        "        #Change Concentration\n",
        "        changes_per_file = [\n",
        "            sum(hunk.added + hunk.removed for hunk in file)\n",
        "            for file in patch\n",
        "        ]\n",
        "\n",
        "        if changes_per_file:\n",
        "            max_file_change = max(changes_per_file)\n",
        "            mean_change = np.mean(changes_per_file)\n",
        "            change_concentration = max_file_change / (total_lines_changed + 1)\n",
        "            change_spread = np.std(changes_per_file) / (mean_change + 1)\n",
        "        else:\n",
        "            max_file_change = change_concentration = change_spread = 0\n",
        "\n",
        "        return pd.Series([\n",
        "            num_files, num_hunks, lines_added, lines_removed,\n",
        "            total_lines_changed, change_ratio,\n",
        "            max_file_change, change_concentration, change_spread\n",
        "        ])\n",
        "\n",
        "    except Exception:\n",
        "        return pd.Series([None] * 9)\n",
        "\n",
        "\n",
        "target_columns = [\n",
        "    \"num_files\", \"num_hunks\", \"lines_added\", \"lines_removed\",\n",
        "    \"total_lines_changed\", \"change_ratio\",\n",
        "    \"max_file_change\", \"change_concentration\",\"change_spread\"\n",
        "]\n",
        "\n",
        "df_cleaned[target_columns] = df_cleaned[\"patch\"].apply(extract_patch_features)\n"
      ],
      "metadata": {
        "id": "mN3lkMaUISNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Full feature set\n",
        "comprehensive_numerical_cols = [\n",
        "    \"num_files\", \"num_hunks\", \"lines_added\", \"lines_removed\",\n",
        "    \"total_lines_changed\", \"change_ratio\",\n",
        "    \"max_file_change\", \"change_concentration\", \"change_spread\"\n",
        "]\n",
        "\n",
        "#Selected features for SWE-bench verified clustering\n",
        "selected_numerical_cols = [\n",
        "    \"num_files\",               # Multi-file changes\n",
        "    \"num_hunks\",               # Complexity indicator\n",
        "    \"total_lines_changed\",     # Overall patch size\n",
        "    \"change_ratio\",            # Add/removed line balance\n",
        "    \"max_file_change\",         # Concentration of changes\n",
        "    \"change_concentration\"     # How focused the changes are\n",
        "]"
      ],
      "metadata": {
        "id": "XKOZPOCwRmf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping redundant features\n",
        "cols_to_drop_lowentropy = [\"lines_added\"] + [\"lines_removed\"] + [\"change_spread\"] \\\n",
        "\n",
        "df_cleaned = df_cleaned.drop(columns = cols_to_drop_lowentropy)"
      ],
      "metadata": {
        "id": "FdCpuYrYSIwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.head(5)"
      ],
      "metadata": {
        "id": "nC2blItNLo19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.columns"
      ],
      "metadata": {
        "id": "GJ7Ll3a-92vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unified Clustering\n"
      ],
      "metadata": {
        "id": "pk3YoWaVBIVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating the numerical, ordinal, categorical and text embeddings for the unified clustering approach"
      ],
      "metadata": {
        "id": "zvOkpVoFjExW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical"
      ],
      "metadata": {
        "id": "A769wWvZjMsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling the numerical features\n",
        "numerical_cols = [\"num_files\", \"num_hunks\", \"total_lines_changed\",\n",
        "                  \"change_ratio\", \"max_file_change\", \"change_concentration\"]\n",
        "scaler = RobustScaler()\n",
        "numerical_features = scaler.fit_transform(df_cleaned[numerical_cols])"
      ],
      "metadata": {
        "id": "BFTKmxq-kAzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ordinal and Categorical"
      ],
      "metadata": {
        "id": "_VwGfUU6jOvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique difficulty values:\", df_cleaned[\"difficulty\"].unique())"
      ],
      "metadata": {
        "id": "_4-_71rUmvpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique underspecified values:\", df_cleaned['underspecified'].unique())"
      ],
      "metadata": {
        "id": "ymmtNjjInfvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "difficulty_map = {\"<15 min fix\": 0, \"15 min - 1 hour\": 1, \"1-4 hours\": 2, \">4 hours\": 3}\n",
        "difficulty_features = df_cleaned['difficulty'].map(difficulty_map).values.reshape(-1, 1)\n",
        "underspecified_features = df_cleaned['underspecified'].values.reshape(-1, 1).astype(float)"
      ],
      "metadata": {
        "id": "hjCs-BUslpQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text embeddings using BERT"
      ],
      "metadata": {
        "id": "s5E5FbCNkJSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_features(df, include_repo_context=True):\n",
        "    \"\"\"This function extracts BERT embeddings from the problem statements with adding optional repo context\"\"\"\n",
        "\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    if include_repo_context:\n",
        "        #repo context mapping\n",
        "        repo_context = {\n",
        "            'astropy/astropy': 'astronomy scientific computing',\n",
        "            'django/django': 'web framework backend',\n",
        "            'matplotlib/matplotlib': 'data visualization plotting',\n",
        "            'mwaskom/seaborn': 'statistical visualization',\n",
        "            'pallets/flask': 'web microframework',\n",
        "            'psf/requests': 'HTTP library networking',\n",
        "            'pydata/xarray': 'multidimensional arrays',\n",
        "            'pylint-dev/pylint': 'code analysis linting',\n",
        "            'pytest-dev/pytest': 'testing framework',\n",
        "            'scikit-learn/scikit-learn': 'machine learning',\n",
        "            'sphinx-doc/sphinx': 'documentation generator',\n",
        "            'sympy/sympy': 'symbolic mathematics'\n",
        "        }\n",
        "\n",
        "        #creating domain-aware problem statements\n",
        "        df['repo_context'] = df['repo'].map(repo_context).fillna(df['repo'].str.split('/').str[-1])\n",
        "        problem_statements = ('[' + df['repo_context'] + '] ' + df['problem_statement'].fillna('')).tolist()\n",
        "\n",
        "    text_embeddings = model.encode(problem_statements, show_progress_bar=True)\n",
        "\n",
        "    return text_embeddings\n",
        "\n",
        "#extracting with repo context\n",
        "text_features = extract_text_features(df_cleaned, include_repo_context=True)"
      ],
      "metadata": {
        "id": "rpLefeFOugUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining features"
      ],
      "metadata": {
        "id": "wSFfVwrGt1ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combining all features\n",
        "combined_features = np.hstack([\n",
        "    numerical_features,\n",
        "    difficulty_features,\n",
        "    underspecified_features,\n",
        "    text_features\n",
        "])\n",
        "\n",
        "print(f\"Numerical shape: {numerical_features.shape}\")\n",
        "print(f\"Difficulty shape: {difficulty_features.shape}\")\n",
        "print(f\"Underspecified shape: {underspecified_features.shape}\")\n",
        "print(f\"Text shape: {text_features.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RONtkUBhlpit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalising for spherical clustering\n",
        "spherical_features = normalize(combined_features, norm='l2')\n",
        "print(f\"Normalized features shape: {spherical_features.shape}\")"
      ],
      "metadata": {
        "id": "bT7h8pSJlpmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Elbow method and silhouette analysis\n",
        "k_range = range(2, 21)  #testing k from 2-20\n",
        "wcss = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=50)\n",
        "    clusters = kmeans.fit_predict(spherical_features)\n",
        "\n",
        "    wcss.append(kmeans.inertia_)\n",
        "    sil_score = silhouette_score(spherical_features, clusters)\n",
        "    silhouette_scores.append(sil_score)\n",
        "\n",
        "#plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1.plot(k_range, wcss, 'bo-')\n",
        "ax1.set_xlabel('Number of clusters (k)',fontsize = 12)\n",
        "ax1.set_ylabel('WCSS', fontsize = 12)\n",
        "ax1.set_title('Elbow Method')\n",
        "\n",
        "ax2.plot(k_range, silhouette_scores, 'ro-')\n",
        "ax2.set_xlabel('Number of clusters (k)',fontsize = 12)\n",
        "ax2.set_ylabel('Silhouette Score',fontsize = 12)\n",
        "ax2.set_title('Silhouette Analysis')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lmPkKKf6lpqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "k_range = range(2, 21)\n",
        "metrics_for_thesis = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=50)\n",
        "    labels = kmeans.fit_predict(spherical_features)\n",
        "\n",
        "    sil = silhouette_score(spherical_features, labels)\n",
        "    db = davies_bouldin_score(spherical_features, labels)\n",
        "\n",
        "    metrics_for_thesis.append({\n",
        "        'k': k,\n",
        "        'silhouette': sil,\n",
        "        'davies_bouldin': db\n",
        "    })\n",
        "\n",
        "#identification of best k\n",
        "best_k_sil = max(metrics_for_thesis, key=lambda x: x['silhouette'])['k']\n",
        "best_k_db = min(metrics_for_thesis, key=lambda x: x['davies_bouldin'])['k']\n",
        "\n",
        "print(f\"Best k by Silhouette: {best_k_sil}\")\n",
        "print(f\"Best k by Davies-Bouldin: {best_k_db}\")"
      ],
      "metadata": {
        "id": "6xdpjToNx-Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
        "from scipy.stats import entropy\n",
        "\n",
        "def evaluate_clustering_for_thesis(features, labels, df, k):\n",
        "    \"\"\"Function to calculate the cluster validation metrics\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    #1. Connectivity\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    nn = NearestNeighbors(n_neighbors=5)\n",
        "    nn.fit(features)\n",
        "    distances, indices = nn.kneighbors(features)\n",
        "\n",
        "    connectivity = 0\n",
        "    for i, neighbors in enumerate(indices):\n",
        "        same_cluster = sum(labels[n] == labels[i] for n in neighbors[1:])\n",
        "        connectivity += same_cluster / (len(neighbors) - 1)\n",
        "    results['connectivity'] = connectivity / len(features)\n",
        "\n",
        "    #2. Dunn Index\n",
        "    from sklearn.metrics.pairwise import pairwise_distances\n",
        "    unique_labels = np.unique(labels)\n",
        "\n",
        "    inter_distances = []\n",
        "    for i in range(len(unique_labels)):\n",
        "        for j in range(i+1, len(unique_labels)):\n",
        "            mask_i = labels == unique_labels[i]\n",
        "            mask_j = labels == unique_labels[j]\n",
        "            dist = pairwise_distances(features[mask_i], features[mask_j]).min()\n",
        "            inter_distances.append(dist)\n",
        "\n",
        "    intra_distances = []\n",
        "    for label in unique_labels:\n",
        "        mask = labels == label\n",
        "        if mask.sum() > 1:\n",
        "            cluster_features = features[mask]\n",
        "            dist = pairwise_distances(cluster_features).max()\n",
        "            intra_distances.append(dist)\n",
        "\n",
        "    results['dunn_index'] = min(inter_distances) / max(intra_distances) if inter_distances and intra_distances else 0\n",
        "\n",
        "    #3. Repo NMI\n",
        "    repo_labels = pd.Categorical(df['repo']).codes\n",
        "    results['repo_nmi'] = normalized_mutual_info_score(labels, repo_labels)\n",
        "\n",
        "    #4. Specification NMI\n",
        "    spec_labels = df['underspecified'].astype(int).values\n",
        "    results['spec_nmi'] = normalized_mutual_info_score(labels, spec_labels)\n",
        "\n",
        "    # 5. Difficulty NMI\n",
        "    diff_map = {'<15 min fix': 0, '15 min - 1 hour': 1, '1-4 hours': 2, '>4 hours': 3}\n",
        "    diff_labels = df['difficulty'].map(diff_map).values\n",
        "    results['diff_nmi'] = normalized_mutual_info_score(labels, diff_labels)\n",
        "\n",
        "\n",
        "    #5. Stability\n",
        "    stability_scores = []\n",
        "    for seed in range(42, 52):  #10 runs\n",
        "        kmeans_temp = KMeans(n_clusters=k, random_state=seed, n_init=10)\n",
        "        labels_temp = kmeans_temp.fit_predict(features)\n",
        "        stability_scores.append(adjusted_rand_score(labels, labels_temp))\n",
        "\n",
        "    results['stability'] = np.mean(stability_scores)\n",
        "    results['stability_std'] = np.std(stability_scores)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "results_list = []\n",
        "\n",
        "for k in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=50)\n",
        "    labels = kmeans.fit_predict(spherical_features)\n",
        "    metrics = evaluate_clustering_for_thesis(spherical_features, labels, df_cleaned, k)\n",
        "\n",
        "    results_list.append({\n",
        "        'K': k,\n",
        "        'Connectivity': f\"{metrics['connectivity']:.1%}\",\n",
        "        'Dunn Index': f\"{metrics['dunn_index']:.3f}\",\n",
        "        'Specification NMI': f\"{metrics['spec_nmi']:.3f}\",\n",
        "        'Difficulty NMI': f\"{metrics['diff_nmi']:.3f}\",\n",
        "        'Repository NMI': f\"{metrics['repo_nmi']:.3f}\",\n",
        "        'Stability': f\"{metrics['stability']:.3f} ± {metrics['stability_std']:.3f}\"\n",
        "    })\n",
        "\n",
        "#creating df\n",
        "metrics_df = pd.DataFrame(results_list)\n",
        "print(\"\\n=== Alternative Clustering Metrics ===\")\n",
        "metrics_df\n"
      ],
      "metadata": {
        "id": "GQyXYKQt0Rca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cluster sizes with k=6\n",
        "spherical_kmeans = KMeans(n_clusters=6, random_state=42, n_init=50)\n",
        "unified_clusters = spherical_kmeans.fit_predict(spherical_features)\n",
        "\n",
        "df_cleaned['unified_cluster'] = unified_clusters\n",
        "print(\"Cluster distribution:\")\n",
        "print(df_cleaned['unified_cluster'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "A8UWUu6Qlp0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#creating t-SNE visualisation\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "tsne_results = tsne.fit_transform(spherical_features)\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1],\n",
        "                     c=df_cleaned['unified_cluster'],\n",
        "                     cmap='tab10',\n",
        "                     alpha=0.6,\n",
        "                     s=50)\n",
        "\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.xlabel('t-SNE Component 1', fontsize=12)\n",
        "plt.ylabel('t-SNE Component 2',fontsize=12)\n",
        "plt.title('t-SNE Visualisation of Unified Spherical Clusters (K=6)')\n",
        "\n",
        "#adding cluster centers (approximates)\n",
        "for i in range(6):\n",
        "    cluster_points = tsne_results[df_cleaned['unified_cluster'] == i]\n",
        "    center_x = cluster_points[:, 0].mean()\n",
        "    center_y = cluster_points[:, 1].mean()\n",
        "    plt.annotate(f'C{i}', (center_x, center_y),\n",
        "                fontsize=12, fontweight='bold',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2OTPV9Djy5Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analysing cluster composition by repos\n",
        "cluster_repo_dist = pd.crosstab(df_cleaned['repo'], df_cleaned['unified_cluster'])\n",
        "print(\"Cluster composition by repository:\")\n",
        "print(cluster_repo_dist)\n",
        "\n",
        "#checking if clusters align with domains\n",
        "for cluster in range(6):\n",
        "    mask = df_cleaned['unified_cluster'] == cluster\n",
        "    repos = df_cleaned[mask]['repo'].value_counts()\n",
        "    print(f\"\\nCluster {cluster} top repositories:\")\n",
        "    print(repos.head())"
      ],
      "metadata": {
        "id": "gRFdohXwv7-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cluster_summary_table(df):\n",
        "    \"\"\"function creates summary table using mean values\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for cluster_num in range(6):\n",
        "        subset = df[df['unified_cluster'] == cluster_num]\n",
        "\n",
        "        results.append({\n",
        "            'Cluster': cluster_num,\n",
        "            'Size': len(subset),\n",
        "            'Files': subset['num_files'].mean(),\n",
        "            'Hunks': subset['num_hunks'].mean(),\n",
        "            'Total_lines': subset['total_lines_changed'].mean(),\n",
        "            'Change_ratio': subset['change_ratio'].mean(),\n",
        "            'Max_file': subset['max_file_change'].mean(),\n",
        "            'Change_conc': subset['change_concentration'].mean(),\n",
        "            'Underspec_%': subset['underspecified'].mean() * 100,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "summary_table = create_cluster_summary_table(df_cleaned)\n",
        "summary_table"
      ],
      "metadata": {
        "id": "XGEW1QnWW6uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_difficulty_distribution_table(df):\n",
        "    \"\"\"Function to create a table showing difficulty distribution across clusters\"\"\"\n",
        "\n",
        "    difficulty_order = ['<15 min fix', '15 min - 1 hour', '1-4 hours', '>4 hours']\n",
        "    results = []\n",
        "\n",
        "    for cluster_num in range(6):\n",
        "        subset = df[df['unified_cluster'] == cluster_num]\n",
        "        difficulty_counts = subset['difficulty'].value_counts()\n",
        "\n",
        "        row = {'Cluster': cluster_num, 'Size': len(subset)}\n",
        "        #adding counts and percentages\n",
        "        for difficulty_level in difficulty_order:\n",
        "            count = difficulty_counts.get(difficulty_level, 0)\n",
        "            percentage = (count / len(subset)) * 100 if len(subset) > 0 else 0\n",
        "            row[difficulty_level] = f\"{count} ({percentage:.0f}%)\"\n",
        "\n",
        "        #adding primary difficulty\n",
        "        row['Primary'] = difficulty_counts.index[0] if len(difficulty_counts) > 0 else \"N/A\"\n",
        "        results.append(row)\n",
        "\n",
        "    diff_df = pd.DataFrame(results)\n",
        "\n",
        "    #formatted table\n",
        "    print(\"\\nDifficulty Distribution Across Clusters\")\n",
        "    print(\"=\"*120)\n",
        "    print(f\"{'Cluster':<8} {'Size':<6} {'<15 min':<15} {'15min-1h':<15} {'1-4h':<15} {'>4h':<15} {'Primary':<20}\")\n",
        "    print(\"-\"*120)\n",
        "\n",
        "    for _, row in diff_df.iterrows():\n",
        "        print(f\"{row['Cluster']:<8} {row['Size']:<6} {row['<15 min fix']:<15} {row['15 min - 1 hour']:<15} \"\n",
        "              f\"{row['1-4 hours']:<15} {row['>4 hours']:<15} {row['Primary']:<20}\")\n",
        "\n",
        "    return diff_df\n",
        "\n",
        "difficulty_table = create_difficulty_distribution_table(df_cleaned)"
      ],
      "metadata": {
        "id": "T-22EzqXYiPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_cluster_examples_for_table(df, cluster_num):\n",
        "    \"\"\"This function extracts full problem details for manual selection\"\"\"\n",
        "    subset = df[df['unified_cluster'] == cluster_num]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CLUSTER {cluster_num} - Full Examples\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    #getting representative examples from different difficulty levels\n",
        "    difficulty_counts = subset['difficulty'].value_counts()\n",
        "\n",
        "    for difficulty, count in difficulty_counts.items():\n",
        "        pct = count/len(subset)*100\n",
        "        if pct < 15:  #skipping very minor categories\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n--- {difficulty} ({count} tasks, {pct:.0f}%) ---\")\n",
        "\n",
        "        difficulty_data = subset[subset['difficulty'] == difficulty]\n",
        "        samples = difficulty_data.sample(n=min(5, len(difficulty_data)), random_state=42)  #getting up to 5 examples\n",
        "\n",
        "        for i, (idx, row) in enumerate(samples.iterrows(), 1):\n",
        "            spec_status = \"UNDERSPECIFIED\" if row['underspecified'] else \"WELL-SPECIFIED\"\n",
        "            print(f\"\\n{i}. [{spec_status}] {row['repo']}\")\n",
        "            print(f\"   Lines changed: {int(row['total_lines_changed'])}\")\n",
        "            print(f\"   Problem: {row['problem_statement']}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "#examples for all clusters\n",
        "for cluster_num in range(6):\n",
        "    extract_cluster_examples_for_table(df_cleaned, cluster_num)"
      ],
      "metadata": {
        "id": "ULbXnPnF5EW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT ONLY Clusters"
      ],
      "metadata": {
        "id": "e-Ze3a_4okS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting just BERT embeddings with repo context\n",
        "\n",
        "#repos context mapping (same as before)\n",
        "repo_context = {\n",
        "    'astropy/astropy': 'astronomy scientific computing',\n",
        "    'django/django': 'web framework backend',\n",
        "    'matplotlib/matplotlib': 'data visualization plotting',\n",
        "    'mwaskom/seaborn': 'statistical visualization',\n",
        "    'pallets/flask': 'web microframework',\n",
        "    'psf/requests': 'HTTP library networking',\n",
        "    'pydata/xarray': 'multidimensional arrays',\n",
        "    'pylint-dev/pylint': 'code analysis linting',\n",
        "    'pytest-dev/pytest': 'testing framework',\n",
        "    'scikit-learn/scikit-learn': 'machine learning',\n",
        "    'sphinx-doc/sphinx': 'documentation generator',\n",
        "    'sympy/sympy': 'symbolic mathematics'\n",
        "}\n",
        "\n",
        "#creating domain-aware problem statements\n",
        "df['repo_context'] = df['repo'].map(repo_context).fillna(df['repo'].str.split('/').str[-1])\n",
        "problem_statements = ('[' + df['repo_context'] + '] ' + df['problem_statement'].fillna('')).tolist()\n",
        "\n",
        "#embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "bert_only_embeddings = model.encode(df_cleaned['domain_aware_statement'].tolist(), show_progress_bar=True)\n",
        "\n",
        "#normalising for spherical clustering\n",
        "bert_spherical = normalize(bert_only_embeddings, norm='l2')\n",
        "\n",
        "print(f\"BERT-only embeddings shape: {bert_spherical.shape}\")\n",
        "\n",
        "#testing different k values\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "k_range = range(2, 21)\n",
        "silhouette_scores = []\n",
        "wcss = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=50)\n",
        "    labels = kmeans.fit_predict(bert_spherical)\n",
        "\n",
        "    sil_score = silhouette_score(bert_spherical, labels)\n",
        "    silhouette_scores.append(sil_score)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "    print(f\"k={k}: Silhouette={sil_score:.4f}, WCSS={kmeans.inertia_:.2f}\")\n",
        "\n",
        "#results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1.plot(k_range, wcss, 'bo-')\n",
        "ax1.set_xlabel('Number of clusters (k)')\n",
        "ax1.set_ylabel('WCSS')\n",
        "#ax1.set_title('Elbow Method - BERT Only')\n",
        "\n",
        "ax2.plot(k_range, silhouette_scores, 'ro-')\n",
        "ax2.set_xlabel('Number of clusters (k)')\n",
        "ax2.set_ylabel('Silhouette Score')\n",
        "#ax2.set_title('Silhouette Analysis - BERT Only')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#checking specification NMI for key k values\n",
        "print(\"\\n=== Checking BERT-only Specification NMI ===\")\n",
        "for k in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=50)\n",
        "    labels = kmeans.fit_predict(bert_spherical)\n",
        "\n",
        "    spec_nmi = normalized_mutual_info_score(labels, df_cleaned['underspecified'].astype(int))\n",
        "    print(f\"k={k}: Specification NMI = {spec_nmi:.3f}\")"
      ],
      "metadata": {
        "id": "xu6heFwAqpk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if BERT clusters align with repositories instead\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "print(\"=== BERT-only Repository Alignment ===\")\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=50)\n",
        "    labels = kmeans.fit_predict(bert_spherical)\n",
        "\n",
        "    #repo labels\n",
        "    repo_labels = pd.Categorical(df_cleaned['repo']).codes\n",
        "\n",
        "    repo_nmi = normalized_mutual_info_score(labels, repo_labels)\n",
        "    print(f\"k={k}: Repository NMI = {repo_nmi:.3f}\")\n",
        "\n",
        "#check average pairwise similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarities = cosine_similarity(bert_spherical)\n",
        "avg_similarity = (similarities.sum() - len(similarities)) / (len(similarities) * (len(similarities) - 1))\n",
        "print(f\"\\nAverage pairwise cosine similarity: {avg_similarity:.3f}\")"
      ],
      "metadata": {
        "id": "NIWthiVGqpql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking repos alignment for full k=6 clustering\n",
        "repo_labels = pd.Categorical(df_cleaned['repo']).codes\n",
        "repo_nmi_full = normalized_mutual_info_score(df_cleaned['unified_cluster'], repo_labels)\n",
        "print(f\"Full model (k=6) Repository NMI: {repo_nmi_full:.3f}\")\n",
        "\n",
        "#cross-tab\n",
        "repo_dist = pd.crosstab(df_cleaned['repo'], df_cleaned['unified_cluster'], normalize='columns')\n",
        "print(\"\\nRepository distribution across clusters (column %):\")\n",
        "print(repo_dist.round(2))"
      ],
      "metadata": {
        "id": "XAo1iQGaqpvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}